{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_tensorflow_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zpgeng/Machine-Perception/blob/master/5_tensorflow_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h0ysDGQVNrP",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Tutorial - RNNs\n",
        "\n",
        "In this tutorial, you learn how to build a recurrent neural network (RNN) to model time series data. More specifically, we will implement a model that generates text by predicting one character at a time. This model is widely based on this well-known [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. It is highly recommended to read through the post, as it is a great read. Additionally, we borrow some code from an implementation of this post in TensorFlow, taken from [here](https://github.com/sherjilozair/char-rnn-tensorflow).\n",
        "\n",
        "In the following, we assume familiarity with the TensorFlow tutorial presented in previous tutorials, i.e., you should be aware of TensorFlow's core concepts, such as graph, session, input placeholders, etc. Furthermore, we will not demonstrate any usage of TensorBoard here - for this, refer to the tutorial on CNNs.\n",
        "\n",
        "This tutorial consists of:\n",
        "  1. Introduction to RNNs\n",
        "  2. A Look at the Data\n",
        "  3. Building the Model\n",
        "  4. Generating Text\n",
        "  5. Concluding Remarks and Exercises\n",
        "  6. _[Appendix]_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i15nAgGOVNrS",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to RNNs\n",
        "### Learning from Sequences\n",
        "A recurrent neural network (RNN) is a specialized architecture designed to model time series and sequence data. A question often associated with sequences is: Given a number of time steps in the sequence, what is the most likely next outcome? In other words, we want the model to output the following probability\n",
        "$$\n",
        "p(\\mathbf{x}_t | \\mathbf{x}_0, \\dotsc, \\mathbf{x}_{t-1})\n",
        "$$\n",
        "\n",
        "where $t$ is the index over time. This is also the task we want to solve in this tutorial, namely, given a sequence of characters, what is the most likely next character?\n",
        "\n",
        "To answer the above question, it is a good idea to keep some sort of memory as we walk along the sequence because previous observations in the past influence the desired outcome in the future. As an example, consider predicting the last character in the following sentence (indicated by the question mark):\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/TKfgH7f.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "For us, it is obvious that we should end it with double quotes. However, the model must _remember_ that there was an opening quotation mark that hasn't been closed yet. It gets even more trickier in the following example:\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/iQPGehZ.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "To complete this sentence, the model must not only realize that the missing word is a noun, but it must also remember that we were talking about Italy in the beginning of the sentence and that the capital of Italy is Rome. To achieve this, it is not enough that the model only knows what characters are. It must have some notion of more abstract concepts of the underlying language - it has to learn how characters are formed to create words and how words are structured to build sentences and so on. Learning to understand text on all these different levels is difficult and being able to capture long-term dependencies is essential for such a task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPxeH68MVNrU",
        "colab_type": "text"
      },
      "source": [
        "### Vanilla RNNs\n",
        "In neural networks we model time-dependencies by introducing connections that loop back to the same node (hence the name _recurrent_). The recurrent connection typically updates the internal state/memory of the cell. Such an architecture can be drawn as is shown in the following ([source](http://www.deeplearningbook.org/contents/rnn.html)).\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/Uo6PCfN.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "On the left you see the compact version of the graph. The model takes as input $\\mathbf{x}$ and processes it over time while the recurrent connection updates the internal state $\\mathbf{h}$ located in the memory cell. On the right side you see the unfolded version of the same graph, where we basically discretize the time dimension and make every time step explicit. This \"unfolding\" or \"unrolling\" of an RNN is what we have to do in practice when training. This step makes the model a finite, computational graph that we can actually work with.\n",
        "\n",
        "The model shown above is a bit too simplistic - it just processes the input over time, but does not produce an output. A more realistic RNN looks like this ([source](http://www.deeplearningbook.org/contents/rnn.html)):\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/nLKmj8v.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "Let's have a look at that in more detail. $\\mathbf{x}$ is the input to the model, $\\mathbf{o}$ is the output, and $L$ is a loss function that measures how much the output deviates from the target $\\mathbf{y}$. In our case, $\\mathbf{x}$ is a sequence of characters and the model produces character $\\mathbf{o}_t$ at every time step $t$. We compare it to the target character $\\mathbf{y}_t$ which is equivalent to the next character $\\mathbf{x}_{t+1}$ in our sequence. Note that instead of outputting one specific character, we rather produce a probability distribution over all characters in the vocabulary (more on this later). This is similar to what we did with the CNN for image classification where we produce a probability of belonging to a class, instead of making a hard assignment.\n",
        "\n",
        "The real magic of RNNs happens in the recurrent cell, which we sometimes also call _memory cell_ because it tracks the memory, or hidden state, $\\mathbf{h}$. The question is now, how do we update the state $\\mathbf{h}$? In the vanilla formulation we model it as follows:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{a}^{(t)} &= \\mathbf{W} \\cdot \\mathbf{h}^{(t-1)} + \\mathbf{U} \\cdot \\mathbf{x}^{(t)} + \\mathbf{b}\\\\\n",
        "\\mathbf{h}^{(t)} &= \\tanh(\\mathbf{a}^{(t)})\\\\\n",
        "\\mathbf{o}^{(t)} &= \\mathbf{V} \\cdot \\mathbf{h}^{(t)} + \\mathbf{c}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Here the matrices $\\mathbf{W}$, $\\mathbf{U}$, $\\mathbf{V}$, and biases $\\mathbf{b}$ and $\\mathbf{c}$ represent the learnable parameters of this model. Importantly, those parameters are _shared_ between time steps, i.e. every time step gets exactly the same copy of weights to work with.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "9r4AS4PWVNrV",
        "colab_type": "text"
      },
      "source": [
        "### LSTM Cells\n",
        "Despite its seeming simplicity, the vanilla RNN is already a powerful model. The only problem is that it turned out to be quite difficult to train such an RNN in practice. The reason for this is known as the _vanishing or exploding gradients problem_, which has been introduced in the lecture. In short, when we optimize the weights of an RNN, we end up backpropagating gradients through time. Because of the chain rule, gradients that arrive at layer $t$ are the product of a bunch of gradients of the layers from $t+1$ to $\\tau$ (assuming we unfold the RNN for $\\tau$ time steps in total). Now if each gradient in this large product is small (or big), the multiplication will make the resulting gradient even smaller (or bigger). This is especially a problem for \"early\" layers and if $\\mathbf{\\tau}$ is large, i.e., if we want to capture long-term dependencies. If you would like to read more about this, you can find a great article [here](http://neuralnetworksanddeeplearning.com/chap5.html).\n",
        "\n",
        "So, what can we do to alleviate the problem of unstable gradients in RNNs? One answer was proposed in the seminal work by [Hochreiter and Schmidhuber, 1997](http://www.bioinf.jku.at/publications/older/2604.pdf) where they introduced Long Short Term Memory (LSTM) cells. These cells were designed to remember information for long periods of time and thus have made training of RNNs considerably easier. The following shows a schematic overview of the inner workings of an LSTM cell ([source](https://codeburst.io/generating-text-using-an-lstm-network-no-libraries-2dff88a3968)): \n",
        "\n",
        "<center><img src=\"https://i.imgur.com/FtcD5eR.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "g9cKSEpoVNrX",
        "colab_type": "text"
      },
      "source": [
        "If you would like to read more about LSTM cells, we highly recommend to read [this excellent post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) from colah's blog. In a nutshell, the most important differences between a vanilla and a LSTM cell are:\n",
        "  - The LSTM cell has two hidden variables instead of just one, the hidden state $\\mathbf{h}$ and the cell state $\\mathbf{c}$. \n",
        "  - Updates to the cell state are carefully protected by three gating functions consisting of a sigmoid layer and an element-wise multiplication (denoted by $\\otimes$ or $\\circ$).\n",
        "  - Notice that the cell state $\\mathbf{c}_{t-1}$ can more or less easily flow through the cell (top line in the above diagram) and thus propagate the information further into the next time step.\n",
        "  \n",
        "LSTMs have made training of recurrent structures much easier and have thus become the de-facto standard in RNNs. Of course, there are more cell types available (which you can find out about for example in colah's blog), but LSTMs are usually a good initial choice for a recurrent architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7WstUtRVNrZ",
        "colab_type": "text"
      },
      "source": [
        "## A Look at the Data\n",
        "Let's now turn to our actual problem of training a character-level language model. Following Andrej Karpathy's [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), we are using the Shakespeare dataset which is just a text file containing some of Shakespeare's work. Here is an excerpt:\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/wPFXbqO.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "What we want to achieve with the model can be summarized in the following diagram ([source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)).\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/yYLPfEz.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "71bjN5aRVNra",
        "colab_type": "text"
      },
      "source": [
        "For the sake of simplicity, this diagram assumes a limited vocabulary of only 4 characters `[h, e, l, o]`. The input to the model is the word \"hello\". On the bottom you can see the input at each time step and a one-hot encoding of each character shown in red. Similarly at the top you find the target characters for each input character and a predicted confidence score shown in blue units. For example, in the first time step the confidence attributed to the letter `e` is 2.2, while the confidence for `o` is 4.1. Ideally, the confidence for the bold numbers in the blue boxes should be high, while for the red numbers it should be low. In the middle part of the diagram, shown in green, are the hidden states of the recurrent cells. Through this layer and the associated hidden state vectors, the information is propagated to future time steps, so that hopefully in the last time step the letter \"o\" will have a high confidence score.\n",
        "\n",
        "We are now going to implement this architecture, but with a larger vocabulary and more training data. To use the Shakespeare data set, we need to preprocess the data, i.e., tokenize it, extract the vocabulary, and create batches of a certain sequence length (depending on how many time steps $\\tau$ we want to unfold the RNN for). To do this, we shamelessly copy the code from [this implementation](https://github.com/sherjilozair/char-rnn-tensorflow)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Oza0ldVNrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import os\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phpiYjE6VNrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextLoader():\n",
        "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_length = seq_length\n",
        "        self.encoding = encoding\n",
        "        self.rng = np.random.RandomState(42)\n",
        "\n",
        "        input_file = os.path.join(data_dir, \"shakespeare.txt\")\n",
        "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
        "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
        "\n",
        "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
        "            print(\"reading text file\")\n",
        "            self.preprocess(input_file, vocab_file, tensor_file)\n",
        "        else:\n",
        "            print(\"loading preprocessed files\")\n",
        "            self.load_preprocessed(vocab_file, tensor_file)\n",
        "        self.create_batches()\n",
        "        self.reset_batch_pointer()\n",
        "\n",
        "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
        "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
        "            data = f.read()\n",
        "        counter = collections.Counter(data)\n",
        "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
        "        self.chars, _ = zip(*count_pairs)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        with open(vocab_file, 'wb') as f:\n",
        "            pickle.dump(self.chars, f)\n",
        "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
        "        np.save(tensor_file, self.tensor)\n",
        "\n",
        "    def load_preprocessed(self, vocab_file, tensor_file):\n",
        "        with open(vocab_file, 'rb') as f:\n",
        "            self.chars = pickle.load(f)\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
        "        self.tensor = np.load(tensor_file)\n",
        "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
        "                                                   self.seq_length))\n",
        "\n",
        "    def create_batches(self):\n",
        "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
        "                                                   self.seq_length))\n",
        "\n",
        "        # When the data (tensor) is too small,\n",
        "        # let's give them a better error message\n",
        "        if self.num_batches == 0:\n",
        "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
        "\n",
        "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
        "        xdata = self.tensor\n",
        "        ydata = np.copy(self.tensor)\n",
        "        ydata[:-1] = xdata[1:]\n",
        "        ydata[-1] = xdata[0]\n",
        "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1),\n",
        "                                  self.num_batches, 1)\n",
        "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1),\n",
        "                                  self.num_batches, 1)\n",
        "\n",
        "    def next_batch(self):\n",
        "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
        "        self.pointer += 1\n",
        "        return x, y\n",
        "\n",
        "    def reset_batch_pointer(self):\n",
        "        self.pointer = 0\n",
        "    \n",
        "    def maybe_new_epoch(self):\n",
        "        if self.pointer >= self.num_batches:\n",
        "            # this is a new epoch\n",
        "            self.reset_batch_pointer()\n",
        "            self.reshuffle()\n",
        "    \n",
        "    def reshuffle(self):\n",
        "        idx = self.rng.permutation(len(self.x_batches))\n",
        "        self.x_batches = [self.x_batches[i] for i in idx]\n",
        "        self.y_batches = [self.y_batches[i] for i in idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5SfsLjbVNrh",
        "colab_type": "text"
      },
      "source": [
        "Let's also define some configuration parameters like we did for the CNN tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACo6_yeWVNri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "tf.app.flags.DEFINE_string(\"data_dir\", \"./\", \"Where the training data is stored\")\n",
        "tf.app.flags.DEFINE_string(\"log_dir\", \"/tmp/tensorflow/shakespeare_rnn/logs\", \"Where to store summaries and checkpoints\")\n",
        "tf.app.flags.DEFINE_float(\"learning_rate\", 1e-3, \"Learning rate (default: 1e-3)\")\n",
        "tf.app.flags.DEFINE_integer(\"batch_size\", 128, \"Batch size (default: 50)\")\n",
        "tf.app.flags.DEFINE_integer(\"seq_length\", 50, \"Number of time steps to unrol the RNN for\")\n",
        "tf.app.flags.DEFINE_integer(\"hidden_size\", 256, \"Size of one LSTM hidden layer\")\n",
        "tf.app.flags.DEFINE_integer(\"num_layers\", 2, \"How many LSTM layers to use\")\n",
        "tf.app.flags.DEFINE_integer(\"print_every_steps\", 20, \"How often to print progress to the console\")\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')  # Dummy entry because colab is weird."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBc-mByqVNrj",
        "colab_type": "code",
        "outputId": "9a31c851-5c70-4c7f-aefe-7fd23354e2ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "FLAGS = tf.app.flags.FLAGS\n",
        "print(\"\\nCommand-line Arguments:\")\n",
        "for key in FLAGS.flag_values_dict():\n",
        "  print(\"{:<22}: {}\".format(key.upper(), FLAGS[key].value))\n",
        "print(\" \")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Command-line Arguments:\n",
            "DATA_DIR              : ./\n",
            "LOG_DIR               : /tmp/tensorflow/shakespeare_rnn/logs\n",
            "LEARNING_RATE         : 0.001\n",
            "BATCH_SIZE            : 128\n",
            "SEQ_LENGTH            : 50\n",
            "HIDDEN_SIZE           : 256\n",
            "NUM_LAYERS            : 2\n",
            "PRINT_EVERY_STEPS     : 20\n",
            "F                     : \n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsQ7x5F_VZB7",
        "colab_type": "code",
        "outputId": "d78fb523-189e-4bdc-daf5-f4b93103c9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!if [ ! -f eye_data.h5 ]; then wget -nv https://ait.ethz.ch/projects/shakespeare.txt?raw=true -O shakespeare.txt; fi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-20 17:12:05 URL:https://ait.ethz.ch/projects/shakespeare.txt?raw=true [1155394/1155394] -> \"shakespeare.txt\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-fvnEROVNrl",
        "colab_type": "code",
        "outputId": "fa411b66-7fc1-45e3-a8da-cd5b5df273a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# load the data\n",
        "data_loader = TextLoader(FLAGS.data_dir, FLAGS.batch_size, FLAGS.seq_length)\n",
        "print(\"loaded vocabulary with {} letters\".format(data_loader.vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading text file\n",
            "loaded vocabulary with 66 letters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX4URLa3VNrn",
        "colab_type": "code",
        "outputId": "ab749ec5-9d0a-43b0-e93b-90a849c926cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# visualize some data\n",
        "b = data_loader.x_batches[0]\n",
        "t = data_loader.y_batches[0]\n",
        "print('total of {} batches of shape: {}'.format(len(data_loader.x_batches), b.shape))\n",
        "print('content of batch 0, entry 0, time steps 0 to 20')\n",
        "print('input : {}'.format(b[0, :20]))\n",
        "print('target: {}'.format(t[0, :20]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total of 180 batches of shape: (128, 50)\n",
            "content of batch 0, entry 0, time steps 0 to 20\n",
            "input : [50  9  7  6  2  0 38  9  2  9 58  1  8 25 10 11 44  1 19  3]\n",
            "target: [ 9  7  6  2  0 38  9  2  9 58  1  8 25 10 11 44  1 19  3  7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NQureGqVNrp",
        "colab_type": "code",
        "outputId": "6596a4cd-ec8f-4201-f872-fe80a3260af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print characters instead of integers\n",
        "# invert the vocabulary (note that this works because the vocabulary is distinct)\n",
        "vocab_inv = {v: k for k, v in data_loader.vocab.items()}\n",
        "print('input : {}'.format([vocab_inv[i] for i in b[0, :20]]))\n",
        "print('target: {}'.format([vocab_inv[i] for i in t[0, :20]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input : ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\r', '\\n', 'B', 'e', 'f', 'o']\n",
            "target: ['i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\r', '\\n', 'B', 'e', 'f', 'o', 'r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFzwPML0VNrs",
        "colab_type": "text"
      },
      "source": [
        "Great - we now have a way of tokenizing text and organizing it into batches of a given size and sequence length. Next, we'll look into how to build the actual RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGYgv8JHVNrt",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model\n",
        "We start by building the core of our model, the RNN with LSTM cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfLr2E7NVNrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_lstm(inputs, hidden_size, num_layers, seq_lengths):\n",
        "    \"\"\"\n",
        "    Builds an RNN with LSTM cells.\n",
        "    :param inputs: The input tensor to the RNN in shape `[batch_size, seq_length]`.\n",
        "    :param hidden_size: The number of units for each LSTM cell.\n",
        "    :param num_layers: The number of LSTM cells we want to use.\n",
        "    :param seq_lengths: Tensor of shape `[batch_size]` specifying the total number\n",
        "      of time steps per sequence.\n",
        "    :return: The initial state, final state, predicted logits and probabilities.\n",
        "    \"\"\"\n",
        "    # we first create a one-hot encoding of the inputs\n",
        "    # the resulting shape is `[batch_size, seq_length, vocab_size]`\n",
        "    vocab_size = data_loader.vocab_size\n",
        "    input_one_hot = tf.one_hot(inputs, vocab_size, axis=-1)\n",
        "    \n",
        "    # create a list of all LSTM cells we want\n",
        "    cells = [tf.contrib.rnn.LSTMCell(hidden_size) for _ in range(num_layers)]\n",
        "    \n",
        "    # we stack the cells together and create one big RNN cell\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "    \n",
        "    # we need to set an initial state for the cells\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "    \n",
        "    # now we are ready to unrol the graph\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell=cell,\n",
        "                                             initial_state=initial_state,\n",
        "                                             inputs=input_one_hot,\n",
        "                                             sequence_length=seq_lengths)\n",
        "    \n",
        "    # The `outputs` tensor has shape `[batch_size, seq_length, hidden_size]`,\n",
        "    # i.e. it contains the outputs of the last cell for every time step.\n",
        "    # We want to map the output back to the \"vocabulary space\", so we add a dense layer.\n",
        "    # Importantly, the dense layer should share its parameters across time steps.\n",
        "    # To do this, we first flatten the outputs to `[batch_size*seq_length, hidden_size]`\n",
        "    # and then add the dense layer.\n",
        "    max_seq_length = tf.shape(inputs)[1]\n",
        "    outputs_flat = tf.reshape(outputs, [-1, hidden_size])\n",
        "    \n",
        "    # dense layer\n",
        "    weights = tf.Variable(tf.truncated_normal([hidden_size, vocab_size], stddev=0.1))\n",
        "    bias = tf.Variable(tf.constant(0.1, shape=[vocab_size]))\n",
        "    logits_flat = tf.matmul(outputs_flat, weights) + bias\n",
        "    \n",
        "    # reshape back\n",
        "    logits = tf.reshape(logits_flat, [batch_size, max_seq_length, vocab_size])\n",
        "    \n",
        "    # activate to turn logits into probabilities\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    \n",
        "    # we return the initial and final states because this will be useful later\n",
        "    return initial_state, final_state, logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us_08WCHVNrv",
        "colab_type": "text"
      },
      "source": [
        "A note about the use of `tf.nn.dynamic_rnn`. When dealing with sequences it is often the case that not all have the same length. Hence, we need to ask ourselves two questions:\n",
        "  1. How do we handle sequences of different lengths in the same batch?\n",
        "  2. Do we want to use different sequence lengths at inference time than at training time?\n",
        "  \n",
        "To answer the first question, we can simply pad all sequences in a batch with dummy values to the maximum length occurring in that batch. Of course, we should tell TensorFlow that it does not make sense to unrol the RNN further for these dummy values. This is why we supply the `seq_lengths` tensor, which is actually important to guarantee correctness during back-propagation. Note that in our example, we do not need to pad the data, because our data loader already ensures that we have sequences of equal length. However, you should generally be aware of that caveat.\n",
        "\n",
        "To address the second question TensorFlow knows two functions: `tf.nn.dynamic_rnn` and `tf.nn.static_rnn`. In the static RNN, TensorFlow creates an unrolled graph for a fixed length (say 100). It is still possible to use this graph for sequences of length `< 100` (by supplying the `seq_lengths` tensor as mentioned above), but during inference time, we cannot use it for more than 100 time steps. The dynamic RNN on the other hand can handle variable sequence lengths - it unrols the graph in a `tf.while` loop directly on the GPU. In other words, the time dimension in the input placeholder can be `None`, like for the batch size. It is recommended to always pass a `seq_lengths` tensor into the dynamic RNN function.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/rTJuTka.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "In the following we have to take care of the remaining tasks: build input placeholders, add a loss function, define the optimizer and define the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9TjPu98VNrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create input placeholders\n",
        "with tf.name_scope(\"input\"):\n",
        "    # shape is `[batch_size, seq_length]`, both are dynamic\n",
        "    text_input = tf.placeholder(tf.int32, [None, None], name='x-input')\n",
        "    # shape of target is same as shape of input\n",
        "    text_target = tf.placeholder(tf.int32, [None, None], name='y-input')\n",
        "    # sequence length placeholder\n",
        "    seq_lengths = tf.placeholder(tf.int32, [None], name='seq-lengths')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7NBT1lmVNrx",
        "colab_type": "code",
        "outputId": "c46470ec-59e1-4daa-d3da-6ae572f21d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# build the model\n",
        "initial_state, final_state, logits, probs = rnn_lstm(text_input,\n",
        "                                                     FLAGS.hidden_size,\n",
        "                                                     FLAGS.num_layers,\n",
        "                                                     seq_lengths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vX8nF7NVNrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We use the same loss function as we did for the CNN tutorial, i.e. Cross-Entropy.\n",
        "# This time, we have to compute it for each time step, so we use a TensorFlow function\n",
        "# that takes care of this for us.\n",
        "with tf.name_scope(\"cross-entropy\"):\n",
        "    # Again, we should supply the logits, not the softmax-activated probabilities\n",
        "    cross_entropy_loss = tf.contrib.seq2seq.sequence_loss(\n",
        "        logits, text_target,\n",
        "        weights=tf.ones_like(text_input, dtype=tf.float32))\n",
        "    tf.summary.scalar('cross_entropy_loss', cross_entropy_loss)\n",
        "    \n",
        "    # The weights allow weighing the contribution of each batch entry and time step (here we don't use it).\n",
        "    # This parameter can be useful if we have padded values (which we don't here). In this case, `weights`\n",
        "    # serves like a mask that you should set to 0 for padded values, e.g. like this:\n",
        "    #   weights = tf.sequence_mask(seq_lengths, max_seq_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSDAGtjzVNr1",
        "colab_type": "code",
        "outputId": "1328a1fd-48ef-4982-88fc-7b6f82ba7f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# check number of trainable parameters\n",
        "def count_trainable_parameters():\n",
        "    \"\"\"Counts the number of trainable parameters in the current default graph.\"\"\"\n",
        "    tot_count = 0\n",
        "    for v in tf.trainable_variables():\n",
        "        v_count = 1\n",
        "        for d in v.get_shape():\n",
        "            v_count *= d.value\n",
        "        tot_count += v_count\n",
        "    return tot_count\n",
        "print(\"Number of trainable parameters: {}\".format(count_trainable_parameters()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 873026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_Jxh4xfVNr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the optimizer\n",
        "global_step = tf.Variable(1, name='global_step', trainable=False)\n",
        "with tf.name_scope(\"train\"):\n",
        "    # choose Adam optimizer\n",
        "    optim = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
        "    \n",
        "    # get the gradients\n",
        "    params = tf.trainable_variables()\n",
        "    gradients = tf.gradients(cross_entropy_loss, params)\n",
        "    \n",
        "    # clip the gradients to counteract exploding gradients\n",
        "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5)\n",
        "    \n",
        "    # backprop\n",
        "    train_step = optim.apply_gradients(zip(clipped_gradients, params), global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edNwfTO-VNr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_train_step(num_steps, summary_op):\n",
        "    \"\"\"Perform as many training steps as specified.\"\"\"\n",
        "    for i in range(num_steps):\n",
        "        step = tf.train.global_step(sess, global_step)\n",
        "        \n",
        "        # reset batch pointer and shuffle the data if necessary\n",
        "        data_loader.maybe_new_epoch()\n",
        "        \n",
        "        # get next batch\n",
        "        x, y = data_loader.next_batch()\n",
        "        \n",
        "        # prepare feed_dict\n",
        "        feed_dict = {text_input: x, text_target: y, seq_lengths: [x.shape[1]]*x.shape[0]}\n",
        "        \n",
        "        summary, train_loss, _ = sess.run([summary_op, cross_entropy_loss, train_step],\n",
        "                                          feed_dict=feed_dict)\n",
        "        \n",
        "        writer_train.add_summary(summary, step)\n",
        "        \n",
        "        if step % FLAGS.print_every_steps == 0:\n",
        "            print('[{}] Cross-Entropy Loss Training [{:.3f}]'.format(step, train_loss)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu-AFYDeVNr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the session\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# To be able to see something in tensorboard, we must merge summaries to one common operation.\n",
        "# Whenever we want to write summaries, we must request this operation from the graph.\n",
        "# Note: creating the file writers should happen after the session was launched.\n",
        "summaries_merged = tf.summary.merge_all()\n",
        "writer_train = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar1db0cnVNr-",
        "colab_type": "text"
      },
      "source": [
        "Now let's train this model for a couple of steps. Make sure you selected the GPU under Runtime > Change runtime type > Hardware Accelerator. Otherwise training will be quite slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO7ntRwEVNr-",
        "colab_type": "code",
        "outputId": "20dea685-6c2a-4eeb-f01e-b75a58bede13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "do_train_step(1001, summaries_merged)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[20] Cross-Entropy Loss Training [3.443]\n",
            "[40] Cross-Entropy Loss Training [3.356]\n",
            "[60] Cross-Entropy Loss Training [3.315]\n",
            "[80] Cross-Entropy Loss Training [3.206]\n",
            "[100] Cross-Entropy Loss Training [3.081]\n",
            "[120] Cross-Entropy Loss Training [2.922]\n",
            "[140] Cross-Entropy Loss Training [2.787]\n",
            "[160] Cross-Entropy Loss Training [2.628]\n",
            "[180] Cross-Entropy Loss Training [2.485]\n",
            "[200] Cross-Entropy Loss Training [2.415]\n",
            "[220] Cross-Entropy Loss Training [2.368]\n",
            "[240] Cross-Entropy Loss Training [2.320]\n",
            "[260] Cross-Entropy Loss Training [2.328]\n",
            "[280] Cross-Entropy Loss Training [2.263]\n",
            "[300] Cross-Entropy Loss Training [2.242]\n",
            "[320] Cross-Entropy Loss Training [2.172]\n",
            "[340] Cross-Entropy Loss Training [2.184]\n",
            "[360] Cross-Entropy Loss Training [2.166]\n",
            "[380] Cross-Entropy Loss Training [2.114]\n",
            "[400] Cross-Entropy Loss Training [2.106]\n",
            "[420] Cross-Entropy Loss Training [2.095]\n",
            "[440] Cross-Entropy Loss Training [2.083]\n",
            "[460] Cross-Entropy Loss Training [2.058]\n",
            "[480] Cross-Entropy Loss Training [2.028]\n",
            "[500] Cross-Entropy Loss Training [2.019]\n",
            "[520] Cross-Entropy Loss Training [2.011]\n",
            "[540] Cross-Entropy Loss Training [1.966]\n",
            "[560] Cross-Entropy Loss Training [1.948]\n",
            "[580] Cross-Entropy Loss Training [1.981]\n",
            "[600] Cross-Entropy Loss Training [1.980]\n",
            "[620] Cross-Entropy Loss Training [1.919]\n",
            "[640] Cross-Entropy Loss Training [1.940]\n",
            "[660] Cross-Entropy Loss Training [1.893]\n",
            "[680] Cross-Entropy Loss Training [1.890]\n",
            "[700] Cross-Entropy Loss Training [1.874]\n",
            "[720] Cross-Entropy Loss Training [1.878]\n",
            "[740] Cross-Entropy Loss Training [1.855]\n",
            "[760] Cross-Entropy Loss Training [1.876]\n",
            "[780] Cross-Entropy Loss Training [1.829]\n",
            "[800] Cross-Entropy Loss Training [1.819]\n",
            "[820] Cross-Entropy Loss Training [1.806]\n",
            "[840] Cross-Entropy Loss Training [1.816]\n",
            "[860] Cross-Entropy Loss Training [1.785]\n",
            "[880] Cross-Entropy Loss Training [1.785]\n",
            "[900] Cross-Entropy Loss Training [1.770]\n",
            "[920] Cross-Entropy Loss Training [1.732]\n",
            "[940] Cross-Entropy Loss Training [1.739]\n",
            "[960] Cross-Entropy Loss Training [1.804]\n",
            "[980] Cross-Entropy Loss Training [1.773]\n",
            "[1000] Cross-Entropy Loss Training [1.752]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxRFXyMHVNsA",
        "colab_type": "text"
      },
      "source": [
        "The loss is consistently decreasing, so that looks promising. Let's now look at another feature from TensorFlow: storing checkpoints. During training, it is a good idea to regularly save the model that you trained up to this point. Doing this with TensorFlow is pretty straight-forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhuRKD6lVNsA",
        "colab_type": "code",
        "outputId": "42453e65-4929-48e9-8a94-9a1f8c2d4104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create a saver object. We must specify which variables it should save to disk (in this case all)\n",
        "# and optionally how many checkpoints should be retained (in this case 2).\n",
        "saver = tf.train.Saver(var_list=tf.trainable_variables(), max_to_keep=2)\n",
        "\n",
        "# new save the current session\n",
        "saver.save(sess, os.path.join(FLAGS.log_dir, 'checkpoints', 'model_name'), global_step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/tmp/tensorflow/shakespeare_rnn/logs/checkpoints/model_name-1002'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ0Zun7IVNsC",
        "colab_type": "text"
      },
      "source": [
        "And that's it! Of course, saving a model is only useful if we can load it again (e.g. to do inference with it or to continue training). This is also quite easy to do. We just call a `restore` function on the saver object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiJ5Xl-LVNsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a handle to the latest checkpoint that was stored\n",
        "ckpt_path = tf.train.latest_checkpoint(os.path.join(FLAGS.log_dir, 'checkpoints'))\n",
        "\n",
        "# now restore\n",
        "saver.restore(sess, ckpt_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QnauOtzVNsF",
        "colab_type": "text"
      },
      "source": [
        "Again, pretty easy. Note however that `saver.restore` only loads the saved weights into the graph, i.e. it assumes a suitable graph exists already. If it does, `restore` will most likely fail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SOKff6oVNsF",
        "colab_type": "text"
      },
      "source": [
        "## Generating Text\n",
        "We have seen how we can train a model to predict a single character given an input sequence. But how can we use this model to generate text? This is what we will discuss in the following.\n",
        "\n",
        "One way to do this is to generate text character-by-character and feeding the output of each time step back as input to the model. In other words, we get the output character for a given sequence, append that character to the sequence and repeat the whole process. This is illustrated in the following where the black text is the input sequence and the blue character is the output character.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/NnToWQe.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "Let's implement this for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlHExL6BVNsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(prime_text, num_steps):\n",
        "    \"\"\"\n",
        "    Sample `num_steps` characters from the model and initialize it with `prime_text`.\n",
        "    :param prime_text: A string that we want to initialize the RNN with.\n",
        "    :num_steps: Integer specifying how many characters we want to predict after `prime_text`.\n",
        "    :return: `prime_text` plus prediction.\n",
        "    \"\"\"\n",
        "    # First we need to look up the initial text in the vocabulary\n",
        "    input_prime = [data_loader.vocab[c] for c in prime_text]\n",
        "    \n",
        "    # Feed the prime sequence into the model. Note that we do not have to supply any targets\n",
        "    # because we are not doing any backpropagation.\n",
        "    feed_dict = {text_input: [input_prime],\n",
        "                 seq_lengths: [len(input_prime)]}\n",
        "    state, out_probs = sess.run([final_state, probs], feed_dict=feed_dict)\n",
        "    \n",
        "    # Now we have initialized the RNN with the given prime text. Let's see what it predicts\n",
        "    # as the next character after the last from `prime_text`.\n",
        "    # `out_probs` is of shape `[1, len(prime_text), vocab_size]`\n",
        "    next_char_probs = out_probs[0, -1]\n",
        "    \n",
        "    # `next_char_probs` is a probability distribution over all characters in the vocabulary.\n",
        "    # How do we determine which character is next? We could just take the one that is most\n",
        "    # probable of course. But let's implement something a bit different: actually sample\n",
        "    # from the probability distribution.\n",
        "    def weighted_pick(p_dist):\n",
        "        cs = np.cumsum(p_dist)\n",
        "        idx = int(np.sum(cs < np.random.rand()))\n",
        "        return idx\n",
        "    \n",
        "    next_char = weighted_pick(next_char_probs)\n",
        "    \n",
        "    # save all predicted chars in a string\n",
        "    predicted_text = vocab_inv[next_char]\n",
        "    \n",
        "    # now we can sample for `num_steps`\n",
        "    for _ in range(num_steps):\n",
        "        # Construct the feed dict. Note how we manually carry over the previous final state\n",
        "        # and use it as the next initial state. If we don't do that, then TensorFlow will\n",
        "        # automatically initialize the cells to the zero state and hence we lose all the\n",
        "        # memory that we've built up to this point.\n",
        "        feed_dict = {text_input: [[next_char]],\n",
        "                     seq_lengths: [1],\n",
        "                     initial_state: state}\n",
        "        \n",
        "        # get the prediction\n",
        "        state, out_probs = sess.run([final_state, probs], feed_dict=feed_dict)\n",
        "        \n",
        "        # sample from the distribution\n",
        "        next_char = weighted_pick(out_probs[0, -1])\n",
        "        \n",
        "        # append to already predicted text\n",
        "        predicted_text += vocab_inv[next_char]   \n",
        "    \n",
        "    return prime_text + predicted_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm7a5Ko0VNsH",
        "colab_type": "code",
        "outputId": "7c918246-14b5-485d-c249-7ac4e0f721f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "print(sample('The ', 500))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The wither of our Gloved\r\n",
            "To well pricones or he' Elike.\r\n",
            "\r\n",
            "All Sore:\r\n",
            "I at the kins gill os is yet,\r\n",
            "And priens; is you with lought hou han, and not me a beill's sreital.\r\n",
            "\r\n",
            "KING EREWAll allow,\r\n",
            "What I wolewn would bruck deathan hall: serist our now,, do.\r\n",
            "\r\n",
            "LOrY CERLENR:\r\n",
            "\r\n",
            "GEONTES:\r\n",
            "Cormond the for.\r\n",
            "\r\n",
            "CApol:\r\n",
            "Nose but the' if your couther wafe,\r\n",
            "Ale, whyos in the itls to the, now how it wors, lit; allen the dreat,\r\n",
            "I dight iols; bue hou the prutcine poor go cance; nece ain weland,\r\n",
            "In thut crust \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV2bPQ5eVNsJ",
        "colab_type": "text"
      },
      "source": [
        "Depending for how long you trained the model, you should now be able to see some nice outputs. As an example, here is the output of a model that was trained for 5000 steps.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/JhMzdEv.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b605aR4SVNsK",
        "colab_type": "text"
      },
      "source": [
        "We make a few observations:\n",
        "  - The model successfully learned to create English words! Even if some might be purely imagined, they do sound at least like English words.\n",
        "  - It also learned a great deal about the structure of the input data: mostly nice use of punctuation, it produces paragraphs that start with names in capital letters, etc.\n",
        "  \n",
        "Given the simplicity of our model, this is quite a nice result! Refer the Andrej Karpathy's [article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) to see some more results of the same model (applied to different datasets) and more visualizations of what's going on inside the RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqMv2iJ1VNsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleanup\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grm0DtpUVNsM",
        "colab_type": "text"
      },
      "source": [
        "## Concluding Remarks and Exercises\n",
        "In this tutorial you learned how to build an RNN that predicts the next character given a sequence of characters and how you can turn it into a generative model that produces sample text of arbitrary length. You should now be aware of the most important implementation details needed to train an RNN in TensorFlow: difference between `tf.nn.static_rnn` and `tf.nn.dynamic_rnn`, potential need for padding, sharing weights when mapping RNN hidden states back to the output space, using initial and final state of the RNN to control the generation of sequences, etc.\n",
        "\n",
        "In our example, we used an RNN to predict an output at each time step of the sequence. RNNs are however much more versatile than this and can be used in many more scenarios as shown here. ([source](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/6L3Pbdc.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>\n",
        "\n",
        "TensorFlow provides [functions](https://www.tensorflow.org/tutorials/seq2seq) to implement such models, which are sometimes also called _sequence-to-sequence_ (seq2seq) models.\n",
        "\n",
        "To gain a deeper understanding of RNNs, we encourage you to make a copy of this notebook and play around with it. Specifically, in the following are a couple of (optional) exercises that you might want to look at. Furthermore, you also find information about some more advanced topics in the appendix section, which we provide for the purpose of self-study.\n",
        "\n",
        "  1. Read [Andrej Karpathy's](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [colah's](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) blog posts.\n",
        "  2. We did not use a validation set in this tutorial. Implement this and evaluate the validation set after every so-many epochs (like we did for the CNN tutorial).\n",
        "  3. Compute the training accuracy, print it to the console and visualize it in TensorBoard. Check the [CNN notebook](https://colab.research.google.com/drive/1bx2dlJYutNitK-hlhp98OHO-5WZnrNyV) to see how you can use Tensorboard on Colab.\n",
        "  4. Try out different cell types other than LSTMs, e.g. GRU.\n",
        "  5. How could you add some regularization to this model? \n",
        "  6. Play around with the hyper-parameters. What happens if you omit the gradient clipping? How susceptible is the training to changes in the learning rate? Can you find a model that has less parameters but performs equally well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvsH1DNdVNsM",
        "colab_type": "text"
      },
      "source": [
        "## Appendix\n",
        "### Writing Your Own RNN Cell\n",
        "Sometimes the standard LSTM cell provided by TensorFlow is just not enough, or sometimes you would like to do fancier stuff within a cell. Knowing how to write your own RNN cell that you can feed into `tf.nn.dynamic_rnn` can be very useful. E.g., we usually want to add a decoder on the outputs of the RNN, i.e. a dense layer that maps back to the output space. In the above model, we did this by reshaping the outputs of the rnn and then adding the dense layer on top of that. It would more elegant however, if we could do this directly in a cell, because anyway the dense layer operates independently on the output of each cell. To do this, we can write a custom RNN cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51PLjv30VNsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.ops.rnn_cell_impl import RNNCell\n",
        "from tensorflow.contrib.rnn import LSTMStateTuple\n",
        "\n",
        "class DenseDecoderCell(RNNCell):\n",
        "    \"\"\"\n",
        "    Wraps an existing RNNCell by decoding the outputs of the cell before returning them.\n",
        "    \"\"\"\n",
        "    def __init__(self, cell, output_dim):\n",
        "        if not isinstance(cell, RNNCell):\n",
        "            raise TypeError(\"The parameter cell is not an RNNCell.\")\n",
        "            \n",
        "        super(DenseDecoderCell, self).__init__()\n",
        "        self._cell = cell\n",
        "        self._output_size = output_dim\n",
        "        \n",
        "        if hasattr(cell.state_size, '__getitem__'):\n",
        "            # `cell` is a MultiRNNCell, so get the size from the last layer\n",
        "            hidden_size = cell.state_size[-1]\n",
        "        else:\n",
        "            hidden_size = cell.state_size\n",
        "            \n",
        "        if isinstance(hidden_size, LSTMStateTuple):\n",
        "            # LSTM cells have special state\n",
        "            hidden_size = hidden_size.h\n",
        "        \n",
        "        # Create the weights of the decoder. However, we must be cautios.\n",
        "        # An instance of this class is created for every time step we unrol\n",
        "        # the model for. Because we want to share parameters between time\n",
        "        # steps, we can't just instantiate new weight variables every time\n",
        "        # this function is called. To avoid this, we use a nice feature\n",
        "        # of TensorFlow which is `tf.get_variable()`. This function creates\n",
        "        # a variable if it does not exist yet or else just retrieves it\n",
        "        # from the graph and returns a handle to it.\n",
        "        print(output_dim)\n",
        "        self._decoder_w = tf.get_variable(\"decoder_cell_w\",\n",
        "                                          shape=[hidden_size, output_dim],\n",
        "                                          dtype=tf.float32,\n",
        "                                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
        "        self._decoder_b = tf.get_variable(\"decoder_cell_b\",\n",
        "                                          shape=[output_dim],\n",
        "                                          dtype=tf.float32,\n",
        "                                          initializer=tf.constant_initializer(0.1))\n",
        "    \n",
        "    @property\n",
        "    def state_size(self):\n",
        "        # just return the state size of the cell we are wrapping\n",
        "        return self._cell.state_size\n",
        "    \n",
        "    @property\n",
        "    def output_size(self):\n",
        "        # must return the dimensionality of the tensor returned by this cell\n",
        "        return self._output_size\n",
        "    \n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        \"\"\"\n",
        "        This is the function that is called at runtime. `inputs` is a tensor of shape\n",
        "        `[batch_size, input_dimension]`, i.e., only one time step for the sequence is given.\n",
        "        `state` is the cell's state from the previous time step and `scope` is just the\n",
        "        current scope (can be ignored most of the time).\n",
        "        \"\"\"\n",
        "        # just forward our call to the cell\n",
        "        output, next_state = self._cell(inputs, state, scope)\n",
        "        \n",
        "        # decode the output\n",
        "        output_dec = tf.matmul(output, self._decoder_w) + self._decoder_b\n",
        "        \n",
        "        return output_dec, next_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtBjTLYmVNsP",
        "colab_type": "text"
      },
      "source": [
        "This way, our original code from above to create an RNN, simplifies a bit. You can use the following function to replace `rnn_lstm` from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayHoLg9uVNsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn_lstm_neat(inputs, hidden_size, num_layers, seq_lengths):\n",
        "    \"\"\"\n",
        "    Builds an RNN with LSTM cells.\n",
        "    :param inputs: The input tensor to the RNN in shape `[batch_size, seq_length]`.\n",
        "    :param hidden_size: The number of units for each LSTM cell.\n",
        "    :param num_layers: The number of LSTM cells we want to use.\n",
        "    :param seq_lengths: Tensor of shape `[batch_size]` specifying the total number\n",
        "      of time steps per sequence.\n",
        "    :return: The initial state, final state, predicted logits and probabilities.\n",
        "    \"\"\"\n",
        "    # we first create a one-hot encoding of the inputs\n",
        "    # the resulting shape is `[batch_size, seq_length, vocab_size]`\n",
        "    vocab_size = data_loader.vocab_size\n",
        "    input_one_hot = tf.one_hot(inputs, vocab_size, axis=-1)\n",
        "    \n",
        "    # create a list of all LSTM cells we want\n",
        "    cells = [tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size) for _ in range(num_layers)]\n",
        "    \n",
        "    # we stack the cells together and create one big RNN cell\n",
        "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
        "    \n",
        "    # decode the outputs\n",
        "    cell = DenseDecoderCell(cell, output_dim=vocab_size)\n",
        "    \n",
        "    # we need to set an initial state for the cells\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "    initial_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
        "    \n",
        "    # now we are ready to unrol the graph\n",
        "    outputs, final_state = tf.nn.dynamic_rnn(cell=cell,\n",
        "                                             initial_state=initial_state,\n",
        "                                             inputs=input_one_hot,\n",
        "                                             sequence_length=seq_lengths)\n",
        "    \n",
        "    # The `outputs` tensor has now shape `[batch_size, seq_length, vocab_size]`,\n",
        "    # so we can directly think of the output as the logits.\n",
        "    logits = outputs\n",
        "    \n",
        "    # activate to turn logits into probabilities\n",
        "    probs = tf.nn.softmax(logits)\n",
        "    \n",
        "    # we return the initial and final states because this will be useful later\n",
        "    return initial_state, final_state, logits, probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsr7wh0CVNsR",
        "colab_type": "text"
      },
      "source": [
        "### Sharing Weights\n",
        "Sharing weights between different structures of the graph is a very useful feature (as seen for example in the previous section about custom RNN cells). TensorFlow easily allows this via the function `tf.get_variable`. Essentially, `tf.get_variable` creates a variable if it does not exist yet or otherwise retrieves it by name from the current graph. Note that the current variable scope influences the name of the variable, so it is important to get the scope right. Here is a simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwHIf5xyVNsS",
        "colab_type": "code",
        "outputId": "ef971796-168b-4969-f8a6-e8115385baf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.variable_scope(\"great_scope\"):\n",
        "        # Variables created here will be named \"great_scope/whatever_name\"\n",
        "        w = tf.get_variable(\"weights\", shape=[256, 10], initializer=tf.constant_initializer(0.1))\n",
        "        \n",
        "print('all variable names', [v.name for v in tf.global_variables()])\n",
        "\n",
        "# create another variable but outside the scope, this will create a new variable because the variable scope\n",
        "# is not set, i.e. `tf.get_variable` searches for a variable named \"weights\" which does not yet exist, so\n",
        "# it creates it\n",
        "w2 = tf.get_variable(\"weights\", shape=[256, 10], initializer=tf.constant_initializer(0.1))\n",
        "print('all variable names (one created)', [v.name for v in tf.global_variables()])\n",
        "\n",
        "# lets retrieve `w`\n",
        "# Note: if we do not set `reuse=True`, TensorFlow will throw an error. This is just to make\n",
        "# sure that you don't unintentionally share variables.\n",
        "with tf.variable_scope(\"great_scope\", reuse=True):\n",
        "        # Variables created here will be named \"great_scope/whatever_name\"\n",
        "        w = tf.get_variable(\"weights\", shape=[256, 10], initializer=tf.constant_initializer(0.1))\n",
        "print('all variable names (none created)', [v.name for v in tf.global_variables()])\n",
        "\n",
        "# If we set `reuse=True` but the variable does not exist, TensorFlow will also throw an error"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all variable names ['great_scope/weights:0']\n",
            "all variable names (one created) ['great_scope/weights:0', 'weights:0']\n",
            "all variable names (none created) ['great_scope/weights:0', 'weights:0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv5ZVKmNVNsU",
        "colab_type": "text"
      },
      "source": [
        "This can get tricky. Newer versions of the API take care of some of this for you. If you'd like to know more about how the sharing of variables works, we recommend reading [this post](https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r1.0/tensorflow/g3doc/how_tos/variable_scope/index.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA8FiD66VNsU",
        "colab_type": "text"
      },
      "source": [
        "### Bi-directional RNNs\n",
        "Bi-directional RNNs (BiRNN) are a powerful variant of recurrent networks. Instead of having only a hidden layer that connects states forward in time, BiRNNs have an additional layer that connects states backward in time. This means, that every time step can draw information from the past as well as the future to produce its prediction. The computational graph of a BiRNN looks something like this ([source](http://www.deeplearningbook.org/contents/rnn.html)):\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/Z1FggLN.png\" align=\"middle\" hspace=\"20px\" vspace=\"5px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rAcTaYdVNsV",
        "colab_type": "text"
      },
      "source": [
        "You can see that both recurrent layers are connected to the output, but they are not connected amongst themselves. For the task of building a character-level language model, we could technically use a BiRNN to predict single characters in gaps (or even more characters). However, it is then not straight-forward to turn this BiRNN into a generative model, because in order to generate predictions, we always need data from the future, too. Hence, the BiRNN - while powerful - is not always suitable for the problem at hand. TensorFlow's API supports the creation of BiRNNs, and it is not much different then creating a uni-directional RNN. In the older API the function is [`tf.compat.v1.nn.bidirectional_dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/bidirectional_dynamic_rnn). Under this link you will find a link to the new API as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUwawhIlEwlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}